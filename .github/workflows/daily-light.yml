name: Daily Refresh (RSS, history, rollups)

on:
  schedule:
    - cron: "30 0 * * *"   # 00:30 UTC daily (≈ 03:30 EAT)
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: ke-top500-daily
  cancel-in-progress: true

defaults:
  run:
    shell: bash

jobs:
  refresh:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          persist-credentials: true
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install Node deps
        run: |
          set -e
          if [ -f package-lock.json ] || [ -f npm-shrinkwrap.json ]; then
            echo "Using npm ci"
            npm ci
          else
            echo "No npm lockfile found -> using npm install"
            npm install
          fi

      - name: Ensure jq (for JSON checks)
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Ensure channels.csv (rebuild if missing or empty)
        run: |
          set -euo pipefail

          line_count() { [ -f "$1" ] && wc -l < "$1" | tr -d ' ' || echo 0; }

          # Prefer ranked CSV if available
          SOURCE_RANKED=""
          if [ -f public/top500_ranked.csv ]; then
            SOURCE_RANKED="public/top500_ranked.csv"
          elif [ -f top500_ranked.csv ]; then
            SOURCE_RANKED="top500_ranked.csv"
          fi

          # Prefer seed list if available
          SEED_FILE=""
          if [ -f scripts/seed_channel_ids.txt ]; then
            SEED_FILE="scripts/seed_channel_ids.txt"
          elif [ -f seed_channel_ids.txt ]; then
            SEED_FILE="seed_channel_ids.txt"
          fi

          rebuild() {
            if [ -n "$SOURCE_RANKED" ]; then
              echo "Building channels.csv from $SOURCE_RANKED"
              python scripts/make_channels_csv.py --ranked "$SOURCE_RANKED" --out channels.csv
            elif [ -n "$SEED_FILE" ]; then
              echo "Building channels.csv from seeds ($SEED_FILE)"
              awk 'BEGIN{print "rank,channel_id,channel_name"} {print NR "," $1 ",Seed Channel " NR}' "$SEED_FILE" > channels.csv
            else
              echo "ERROR: No channels.csv, no ranked CSV, and no seed file found."
              exit 1
            fi
          }

          if [ -f channels.csv ]; then
            if [ "$(line_count channels.csv)" -ge 2 ]; then
              echo "channels.csv present with rows ($(line_count channels.csv) lines)"
            else
              echo "channels.csv present but empty (header-only) → rebuilding…"
              rebuild
            fi
          else
            echo "channels.csv missing → building…"
            rebuild
          fi

          echo "---- channels.csv (first 10 lines) ----"
          head -n 10 channels.csv || true

          if [ "$(line_count channels.csv)" -lt 2 ]; then
            echo "ERROR: channels.csv has no channel rows after rebuild."
            exit 1
          fi

      - name: Ensure public/data folder
        run: mkdir -p public/data

      - name: Refresh latest videos (Hybrid:RSS + API fallback)
        env:
          YT_API_KEY: ${{ secrets.YT_API_KEY }}
          # Keep a conservative fallback to avoid empties when API metadata is missing
          DAILY_FALLBACK_ALLOW_UNKNOWN: "true"
          DAILY_FALLBACK_MAX_AGE_DAYS: "14"
        run: |
          set -e
          node scripts/fetch_latest_top500_hybrid.mjs ./channels.csv ./public/data/top500.json
          echo "---- public/data after daily snapshot ----"
          ls -lah public/data || true
          echo "size top500.json:" && (wc -c public/data/top500.json || true)
          echo "items in top500.json:" && (jq '.items | length' public/data/top500.json || echo "jq failed")
          echo "long-form (>=660s) items:" && (jq '[.items[] | select(.latest_video_duration_sec != null and .latest_video_duration_sec >= 660)] | length' public/data/top500.json || true)

      - name: Append history snapshot
        run: node scripts/append_history.mjs

      - name: Build 7d & 30d rollups (RSS + optional API scoring)
        env:
          YT_API_KEY: ${{ secrets.YT_API_KEY }}
          # Allow unknown durations in rollups too, so they generally remain populated
          ROLLUP_FALLBACK_ALLOW_UNKNOWN: "true"
        run: |
          set -e
          node scripts/make_rollups_from_channels.mjs 7  public/data/top500_7d.json
          node scripts/make_rollups_from_channels.mjs 30 public/data/top500_30d.json
          echo "---- public/data after rollups ----"
          ls -lah public/data || true
          echo "sizes:" && (wc -c public/data/top500.json public/data/top500_7d.json public/data/top500_30d.json || true)
          echo "weekly items:" && (jq '.items | length' public/data/top500_7d.json || true)
          echo "monthly items:" && (jq '.items | length' public/data/top500_30d.json || true)

      - name: RESCUE daily from weekly if empty
        run: |
          set -e
          if [ "$(jq -r '.items | length' public/data/top500.json)" = "0" ]; then
            echo "Daily JSON is empty — synthesizing from weekly rollup (newest per channel)…"
            node -e '
              const fs=require("fs");
              const pathDaily="public/data/top500.json";
              const path7d="public/data/top500_7d.json";
              const d=JSON.parse(fs.readFileSync(pathDaily,"utf8"));
              const w=JSON.parse(fs.readFileSync(path7d,"utf8"));
              const byCh = new Map();
              for (const it of (w.items||[])) {
                const prev = byCh.get(it.channel_id);
                if (!prev || new Date(it.latest_video_published_at) > new Date(prev.latest_video_published_at)) {
                  byCh.set(it.channel_id, it);
                }
              }
              const items = Array.from(byCh.values())
                .sort((a,b)=> new Date(b.latest_video_published_at) - new Date(a.latest_video_published_at));
              const out = { generated_at_utc: new Date().toISOString(), items };
              fs.writeFileSync(pathDaily, JSON.stringify(out,null,2), "utf8");
              console.log("Rescued daily from weekly. items:", items.length);
            '
            echo "items in rescued top500.json:" && (jq '.items | length' public/data/top500.json || true)
            echo "long-form (>=660s) items after rescue:" && (jq '[.items[] | select(.latest_video_duration_sec != null and .latest_video_duration_sec >= 660)] | length' public/data/top500.json || true)
          else
            echo "Daily JSON already has items — no rescue needed."
          fi

      - name: Commit updated artifacts (commit → rebase → push)
        run: |
          set -e
          git config user.name "auto-bot"
          git config user.email "bot@example.com"

          git add channels.csv public/data/top500.json public/data/history.jsonl public/data/top500_7d.json public/data/top500_30d.json || true
          git commit -m "chore: daily refresh + history + rollups [skip ci]" || echo "No changes"

          git fetch origin main
          git pull --rebase --autostash origin main || {
            git stash push -u -m "bot-stash" || true
            git pull --rebase origin main
            git stash pop || true
          }

          git remote set-url origin "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git"
          git push origin HEAD:main
