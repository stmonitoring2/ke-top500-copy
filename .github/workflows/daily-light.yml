name: Daily Refresh (RSS, history, rollups)

on:
  schedule:
    - cron: "30 0 * * *"   # 00:30 UTC daily (≈ 03:30 EAT)
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: ke-top500-daily
  cancel-in-progress: true

defaults:
  run:
    shell: bash

jobs:
  refresh:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          persist-credentials: true
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install Node deps
        run: |
          set -e
          if [ -f package-lock.json ] || [ -f npm-shrinkwrap.json ]; then
            echo "Using npm ci"
            npm ci
          else
            echo "No npm lockfile found -> using npm install"
            npm install
          fi

      - name: Ensure jq (for JSON checks)
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Ensure channels.csv (rebuild if missing or empty)
        run: |
          set -euo pipefail

          line_count() { [ -f "$1" ] && wc -l < "$1" | tr -d ' ' || echo 0; }

          SOURCE_RANKED=""
          if [ -f public/top500_ranked.csv ]; then
            SOURCE_RANKED="public/top500_ranked.csv"
          elif [ -f top500_ranked.csv ]; then
            SOURCE_RANKED="top500_ranked.csv"
          fi

          SEED_FILE=""
          if [ -f scripts/seed_channel_ids.txt ]; then
            SEED_FILE="scripts/seed_channel_ids.txt"
          elif [ -f seed_channel_ids.txt ]; then
            SEED_FILE="seed_channel_ids.txt"
          fi

          rebuild() {
            if [ -n "$SOURCE_RANKED" ]; then
              echo "Building channels.csv from $SOURCE_RANKED"
              python scripts/make_channels_csv.py --ranked "$SOURCE_RANKED" --out channels.csv
            elif [ -n "$SEED_FILE" ]; then
              echo "Building channels.csv from seeds ($SEED_FILE)"
              awk 'BEGIN{print "rank,channel_id,channel_name"} {print NR "," $1 ",Seed Channel " NR}' "$SEED_FILE" > channels.csv
            else
              echo "ERROR: No channels.csv, no ranked CSV, and no seed file found."
              exit 1
            fi
          }

          if [ -f channels.csv ]; then
            if [ "$(line_count channels.csv)" -ge 2 ]; then
              echo "channels.csv present with rows ($(line_count channels.csv) lines)"
            else
              echo "channels.csv present but empty (header-only) → rebuilding…"
              rebuild
            fi
          else
            echo "channels.csv missing → building…"
            rebuild
          fi

          echo "---- channels.csv (first 10 lines) ----"
          head -n 10 channels.csv || true

          if [ "$(line_count channels.csv)" -lt 2 ]; then
            echo "ERROR: channels.csv has no channel rows after rebuild."
            exit 1
          fi

      - name: Ensure public/data folder
        run: mkdir -p public/data

      - name: Refresh latest videos (Hybrid:RSS + API fallback)
        env:
          YT_API_KEY: ${{ secrets.YT_API_KEY }}
          DAILY_FALLBACK_ALLOW_UNKNOWN: "true"
          DAILY_FALLBACK_MAX_AGE_DAYS: "14"
        run: |
          set -e
          node scripts/fetch_latest_top500_hybrid.mjs ./channels.csv ./public/data/top500.json
          echo "---- public/data after daily snapshot ----"
          ls -lah public/data || true
          echo "size top500.json:" && (wc -c public/data/top500.json || true)
          # quick sanity counters for the UI filters:
          echo "items in top500.json:" && (jq '.items | length' public/data/top500.json || echo "jq failed")
          echo ">=660s items:" && (jq '[.items[] | select(.latest_video_duration_sec != null and .latest_video_duration_sec >= 660)] | length' public/data/top500.json || true)
          echo "bad timestamps (non-Z) :" && (jq '[.items[] | select((.latest_video_published_at|tostring) | endswith("Z") | not)] | length' public/data/top500.json || true)

      - name: Append history snapshot
        run: node scripts/append_history.mjs

      - name: Build 7d & 30d rollups (RSS + optional API scoring)
        env:
          YT_API_KEY: ${{ secrets.YT_API_KEY }}
        run: |
          set -e
          node scripts/make_rollups_from_channels.mjs 7  public/data/top500_7d.json
          node scripts/make_rollups_from_channels.mjs 30 public/data/top500_30d.json
          echo "---- public/data after rollups ----"
          ls -lah public/data || true
          echo "sizes:" && (wc -c public/data/top500.json public/data/top500_7d.json public/data/top500_30d.json || true)
          echo "weekly items:" && (jq '.items | length' public/data/top500_7d.json || true)
          echo "monthly items:" && (jq '.items | length' public/data/top500_30d.json || true)

      - name: RESCUE daily from weekly if empty (fill missing durations)
        run: |
          set -e
          if [ "$(jq -r '.items | length' public/data/top500.json)" = "0" ]; then
            echo "Daily JSON is empty — synthesizing from weekly rollup (newest per channel)…"
            node -e '
              const fs = require("fs");

              const MIN_LONGFORM = 660;               // 11 minutes
              const MAX_AGE_DAYS = 90;                // match UI window
              const SHORTS_RE = /(^|\\W)(shorts?|#shorts)(\\W|$)/i;
              const SPORTS_RE = /\\b(highlights?|extended\\s*highlights|FT|full\\s*time|full\\s*match|goal|matchday)\\b|\\b(\\d+\\s*-\\s*\\d+)\\b/i;
              const SENSATIONAL_RE = /(catch(ing)?|expos(e|ing)|confront(ing)?|loyalty\\s*test|loyalty\\s*challenge|pop\\s*the\\s*balloon)/i;
              const MIX_RE = /\\b(dj\\s*mix|dj\\s*set|mix\\s*tape|mixtape|mixshow|party\\s*mix|afrobeat\\s*mix|bongo\\s*mix|kenyan\\s*mix|live\\s*mix)\\b/i;
              const EXTRA_SPORTS = /\\b(sportscast|manchester\\s*united|arsenal|liverpool|chelsea)\\b/i;

              function looksBlocked(title="") {
                return SHORTS_RE.test(title) || SPORTS_RE.test(title) || SENSATIONAL_RE.test(title) || MIX_RE.test(title) || EXTRA_SPORTS.test(title);
              }
              function daysAgo(iso) {
                const t = Date.parse(iso); 
                if (!Number.isFinite(t)) return Infinity;
                return (Date.now() - t) / (1000*60*60*24);
              }

              const dailyPath = "public/data/top500.json";
              const weeklyPath = "public/data/top500_7d.json";
              const d = JSON.parse(fs.readFileSync(dailyPath, "utf8"));
              const w = JSON.parse(fs.readFileSync(weeklyPath, "utf8"));

              const newestByChannel = new Map();
              for (const it of (w.items || [])) {
                if (!it || !it.latest_video_id || !it.latest_video_title) continue;
                if (looksBlocked(it.latest_video_title)) continue;
                if (daysAgo(it.latest_video_published_at) > MAX_AGE_DAYS) continue;

                // Synthesize duration if missing, so UI treats as long-form
                let dur = it.latest_video_duration_sec;
                if (dur == null) dur = MIN_LONGFORM;
                if (dur < MIN_LONGFORM) continue;

                const prev = newestByChannel.get(it.channel_id);
                if (!prev || new Date(it.latest_video_published_at) > new Date(prev.latest_video_published_at)) {
                  newestByChannel.set(it.channel_id, { ...it, latest_video_duration_sec: dur });
                }
              }

              const items = Array.from(newestByChannel.values())
                .sort((a,b) => new Date(b.latest_video_published_at) - new Date(a.latest_video_published_at));

              fs.writeFileSync(dailyPath, JSON.stringify({ generated_at_utc: new Date().toISOString(), items }, null, 2), "utf8");
              console.log("Rescued daily from weekly. items:", items.length);
              const lf = items.filter(x => (x.latest_video_duration_sec ?? 0) >= MIN_LONGFORM).length;
              console.log("Rescued long-form count (>=660s):", lf);
            '
            echo "items in rescued top500.json:" && (jq '.items | length' public/data/top500.json || true)
            echo ">=660s items after rescue:" && (jq '[.items[] | select(.latest_video_duration_sec != null and .latest_video_duration_sec >= 660)] | length' public/data/top500.json || true)
          else
            echo "Daily JSON already has items — no rescue needed."
          fi

      - name: Commit updated artifacts (commit → rebase → push)
        run: |
          set -e
          git config user.name "auto-bot"
          git config user.email "bot@example.com"

          git add channels.csv public/data/top500.json public/data/history.jsonl public/data/top500_7d.json public/data/top500_30d.json || true
          git commit -m "chore: daily refresh + history + rollups [skip ci]" || echo "No changes"

          git fetch origin main
          git pull --rebase --autostash origin main || {
            git stash push -u -m "bot-stash" || true
            git pull --rebase origin main
            git stash pop || true
          }

          git remote set-url origin "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git"
          git push origin HEAD:main
