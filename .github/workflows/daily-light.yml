name: Daily Refresh (RSS, history, rollups)

on:
  schedule:
    - cron: "30 0 * * *"   # 00:30 UTC daily (≈ 03:30 EAT)
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: ke-top500-daily
  cancel-in-progress: true

defaults:
  run:
    shell: bash

jobs:
  refresh:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          persist-credentials: true
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # NOTE: do NOT enable caching here; setup-node's cache requires a lockfile.
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      # Install deps gracefully whether or not a lockfile exists
      - name: Install Node deps
        run: |
          set -e
          if [ -f package-lock.json ] || [ -f npm-shrinkwrap.json ]; then
            echo "Using npm ci"
            npm ci
          else
            echo "No npm lockfile found -> using npm install"
            npm install
          fi

      - name: Ensure jq (for JSON checks)
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Ensure channels.csv (rebuild if missing or empty)
        run: |
          set -euo pipefail

          line_count() { [ -f "$1" ] && wc -l < "$1" | tr -d ' ' || echo 0; }

          # Prefer ranked CSV if available (usually produced by weekly workflow)
          SOURCE_RANKED=""
          if [ -f public/top500_ranked.csv ]; then
            SOURCE_RANKED="public/top500_ranked.csv"
          elif [ -f top500_ranked.csv ]; then
            SOURCE_RANKED="top500_ranked.csv"
          fi

          # Prefer seed list if available
          SEED_FILE=""
          if [ -f scripts/seed_channel_ids.txt ]; then
            SEED_FILE="scripts/seed_channel_ids.txt"
          elif [ -f seed_channel_ids.txt ]; then
            SEED_FILE="seed_channel_ids.txt"
          fi

          rebuild() {
            if [ -n "$SOURCE_RANKED" ]; then
              echo "Building channels.csv from $SOURCE_RANKED"
              python scripts/make_channels_csv.py --ranked "$SOURCE_RANKED" --out channels.csv
            elif [ -n "$SEED_FILE" ]; then
              echo "Building channels.csv from seeds ($SEED_FILE)"
              awk 'BEGIN{print "rank,channel_id,channel_name"} {print NR "," $1 ",Seed Channel " NR}' "$SEED_FILE" > channels.csv
            else
              echo "ERROR: No channels.csv, no ranked CSV, and no seed file found."
              exit 1
            fi
          }

          if [ -f channels.csv ]; then
            if [ "$(line_count channels.csv)" -ge 2 ]; then
              echo "channels.csv present with rows ($(line_count channels.csv) lines)"
            else
              echo "channels.csv present but empty (header-only) → rebuilding…"
              rebuild
            fi
          else
            echo "channels.csv missing → building…"
            rebuild
          fi

          echo "---- channels.csv (first 10 lines) ----"
          head -n 10 channels.csv || true

          # Hard fail if still empty
          if [ "$(line_count channels.csv)" -lt 2 ]; then
            echo "ERROR: channels.csv has no channel rows after rebuild."
            exit 1
          fi

      - name: Ensure public/data folder
        run: mkdir -p public/data

      - name: Refresh latest videos (Hybrid:RSS + API fallback)
        env:
          YT_API_KEY: ${{ secrets.YT_API_KEY }}
          # Conservative fallback if API missing/rate-limited:
          DAILY_FALLBACK_ALLOW_UNKNOWN: "true"
          DAILY_FALLBACK_MAX_AGE_DAYS: "14"
        run: |
          set -e
          node scripts/fetch_latest_top500_hybrid.mjs ./channels.csv ./public/data/top500.json
          echo "---- public/data after daily snapshot ----"
          ls -lah public/data || true
          echo "size top500.json:" && (wc -c public/data/top500.json || true)
          echo "items in top500.json:" && (jq '.items | length' public/data/top500.json || echo "jq failed")

      - name: Rescue retry if empty (broaden fallback window)
        env:
          YT_API_KEY: ${{ secrets.YT_API_KEY }}
        run: |
          set -e
          if [ "$(jq -r '.items | length' public/data/top500.json)" = "0" ]; then
            echo "No items in daily JSON — broadening fallback (MAX_AGE=30d) and retrying…"
            DAILY_FALLBACK_ALLOW_UNKNOWN=true DAILY_FALLBACK_MAX_AGE_DAYS=30 \
              node scripts/fetch_latest_top500_hybrid.mjs ./channels.csv ./public/data/top500.json
            echo "Retry complete. items in top500.json:" && (jq '.items | length' public/data/top500.json || true)
          else
            echo "Daily JSON has items — no rescue needed."
          fi

      - name: Append history snapshot
        run: node scripts/append_history.mjs

      - name: Build 7d & 30d rollups (RSS + optional API scoring)
        env:
          YT_API_KEY: ${{ secrets.YT_API_KEY }}   # durations/views for strict long-form gate
          # Enable the same permissive behavior for rollups when API is tight
          ROLLUP_FALLBACK_ALLOW_UNKNOWN: "true"
        run: |
          set -e
          node scripts/make_rollups_from_channels.mjs 7  public/data/top500_7d.json
          node scripts/make_rollups_from_channels.mjs 30 public/data/top500_30d.json
          echo "---- public/data after rollups ----"
          ls -lah public/data || true
          echo "sizes:" && (wc -c public/data/top500.json public/data/top500_7d.json public/data/top500_30d.json || true)
          echo "weekly items:" && (jq '.items | length' public/data/top500_7d.json || true)
          echo "monthly items:" && (jq '.items | length' public/data/top500_30d.json || true)

      - name: Commit updated artifacts (commit → rebase → push)
        run: |
          set -e
          git config user.name "auto-bot"
          git config user.email "bot@example.com"

          git add channels.csv public/data/top500.json public/data/history.jsonl public/data/top500_7d.json public/data/top500_30d.json || true
          git commit -m "chore: daily refresh + history + rollups [skip ci]" || echo "No changes"

          git fetch origin main
          git pull --rebase --autostash origin main || {
            git stash push -u -m "bot-stash" || true
            git pull --rebase origin main
            git stash pop || true
          }

          git remote set-url origin "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git"
          git push origin HEAD:main
